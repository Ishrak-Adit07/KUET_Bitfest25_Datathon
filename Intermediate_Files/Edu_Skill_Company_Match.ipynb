{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2024-12-28T05:32:06.567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Load the train and test datasets\nnew_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\")\nnew_test_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/test.csv\")\n\n# Check basic structure of the datasets\nprint(\"Train Dataset Overview:\\n\")\n# print(new_train_data.info())\nprint(\"\\nTest Dataset Overview:\\n\")\n# print(new_test_data.info())\n\n# new_train_data['responsibilities']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:05:20.179657Z","iopub.execute_input":"2024-12-27T20:05:20.179948Z","iopub.status.idle":"2024-12-27T20:05:20.336737Z","shell.execute_reply.started":"2024-12-27T20:05:20.179928Z","shell.execute_reply":"2024-12-27T20:05:20.335795Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Overview:\n\n\nTest Dataset Overview:\n\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"unique_values1 = new_train_data['educational_results'].unique()\nunique_values2 = new_train_data['result_types'].unique()\nunique_values3 = new_train_data['educationaL_requirements'].unique()\nunique_values4 = new_train_data['degree_names'].unique()\nunique_values5 = new_train_data['professional_company_names'].unique()\n\n# Print the unique values\n# print(unique_values5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:05:15.141815Z","iopub.execute_input":"2024-12-27T20:05:15.142094Z","iopub.status.idle":"2024-12-27T20:05:15.148407Z","shell.execute_reply.started":"2024-12-27T20:05:15.142073Z","shell.execute_reply":"2024-12-27T20:05:15.147621Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"# Step 1: Handle missing values\nnew_train_data.fillna(\"Unknown\", inplace=True)  # Replace null/None/N/A with \"Unknown\"\nnew_test_data.fillna(\"Unknown\", inplace=True)  # Replace null/None/N/A with \"Unknown\"\n# Alternatively, drop rows/columns with too many missing values\nnew_train_data.dropna(thresh=int(new_train_data.shape[1] * 0.8), axis=0, inplace=True)  # Drop rows with >80% missing\nnew_test_data.dropna(thresh=int(new_train_data.shape[1] * 0.8), axis=0, inplace=True)  # Drop rows with >80% missing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:05:31.638685Z","iopub.execute_input":"2024-12-27T20:05:31.638972Z","iopub.status.idle":"2024-12-27T20:05:31.685281Z","shell.execute_reply.started":"2024-12-27T20:05:31.638951Z","shell.execute_reply":"2024-12-27T20:05:31.684444Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"new_train_data.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:05:36.152471Z","iopub.execute_input":"2024-12-27T20:05:36.152779Z","iopub.status.idle":"2024-12-27T20:05:36.188689Z","shell.execute_reply.started":"2024-12-27T20:05:36.152756Z","shell.execute_reply":"2024-12-27T20:05:36.188008Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nimport re\nimport ast\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Step 1: Define column groups based on type and relevance\ntext_columns = ['address','career_objective','locations', 'extra_curricular_activity_types','extra_curricular_organization_links','online_links']\nnumerical_columns = ['matched_score']\ndate_columns = ['start_dates', 'end_dates', 'issue_dates', 'expiry_dates']\n# Define columns for KNN Imputation\nknn_columns = ['age_requirement','experiencere_requirement']  # Add other columns if needed\n# Get all column names in the DataFrame\nall_columns = set(new_train_data.columns)\n\n# Combine all defined column groups\ndefined_columns = set(text_columns + numerical_columns + date_columns + knn_columns)\n\n# Identify columns that are not in the defined groups\ncategorical_columns = list(all_columns - defined_columns)\n\n# Step 2: Preprocessing Function for Age Column\ndef preprocess_age_requirement(column):\n    # Extract numeric ranges and replace non-numeric with NaN\n    def extract_mean_age(val):\n        if isinstance(val, str):\n            # Find ranges like \"Age 25 to 35 years\" and compute the mean\n            match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if match:\n                return (int(match.group(1)) + int(match.group(2))) / 2\n            # Find single ages like \"Age 25 years\"\n            match = re.search(r'(\\d+)', val)\n            if match:\n                return int(match.group(1))\n        return None  # Return None for non-numeric values\n\n    return column.apply(extract_mean_age)\n\ndef preprocess_experience_requirement(column):\n    \"\"\"\n    Preprocess the experience_requirement column by extracting numeric ranges or single values\n    and replacing non-numeric entries with NaN.\n    \"\"\"\n    def extract_mean_experience(val):\n        if isinstance(val, str):\n            # Find ranges like \"3 to 5 years\" and compute the mean\n            range_match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if range_match:\n                return (int(range_match.group(1)) + int(range_match.group(2))) / 2\n            \n            # Find \"At least X year(s)\" or similar patterns\n            at_least_match = re.search(r'At least (\\d+)', val)\n            if at_least_match:\n                return int(at_least_match.group(1))\n            \n            # Find single experience values like \"1 year\" or \"2 year(s)\"\n            single_match = re.search(r'(\\d+)', val)\n            if single_match:\n                return int(single_match.group(1))\n        \n        # Return None for non-numeric or unprocessable entries\n        return None\n\n    # Apply the extraction logic to the entire column\n    return column.apply(extract_mean_experience)\n\n# Step 3: Preprocess Age Requirement\nnew_train_data['age_requirement'] = preprocess_age_requirement(new_train_data['age_requirement'])\nnew_test_data['age_requirement'] = preprocess_age_requirement(new_test_data['age_requirement'])\nnew_train_data['experiencere_requirement'] = preprocess_experience_requirement(new_train_data['experiencere_requirement'])\nnew_test_data['experiencere_requirement'] = preprocess_experience_requirement(new_test_data['experiencere_requirement'])\n\n# New step for Education qualification matching\ndef compute_similarity(requirements, fields):\n    if not requirements or not fields:\n        return 0.0\n    vectorizer = TfidfVectorizer()\n    all_text = [requirements, fields]\n    tfidf_matrix = vectorizer.fit_transform(all_text)\n    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n    return similarity[0][0]\n\n# Compute educational match\nnew_train_data[\"educational_match\"] = new_train_data.apply(\n    lambda row: compute_similarity(row[\"educationaL_requirements\"], row[\"degree_names\"]), axis=1\n)\nnew_test_data[\"educational_match\"] = new_test_data.apply(\n    lambda row: compute_similarity(row[\"educationaL_requirements\"], row[\"degree_names\"]), axis=1\n)\n\n# Compute field match\nnew_train_data[\"field_match\"] = new_train_data.apply(\n    lambda row: compute_similarity(row[\"educationaL_requirements\"], row[\"major_field_of_studies\"]), axis=1\n)\nnew_test_data[\"field_match\"] = new_test_data.apply(\n    lambda row: compute_similarity(row[\"educationaL_requirements\"], row[\"major_field_of_studies\"]), axis=1\n)\n\n# Cross field match\nnew_train_data[\"edu_field_match\"] = new_train_data[\"educational_match\"] * new_train_data[\"field_match\"]\n\n# Step 4: Impute Categorical Columns\nfor col in categorical_columns:\n    mode_value = new_train_data[col].mode()[0] if not new_train_data[col].mode().empty else \"Not Specified\"\n    new_train_data[col].fillna(mode_value, inplace=True)\n    new_test_data[col].fillna(mode_value, inplace=True)\n\n# Step 5: Impute Text Columns with Placeholder\nfor col in text_columns:\n    new_train_data[col].fillna(\"No Information\", inplace=True)\n    new_test_data[col].fillna(\"No Information\", inplace=True)\n\n\n\nfor col in numerical_columns:\n    if col in new_train_data.columns:  # Check if column exists in train data\n        median_value = new_train_data[col].median()\n        new_train_data[col].fillna(median_value, inplace=True)\n    if col in new_test_data.columns:  # Check if column exists in test data\n        median_value = new_train_data[col].median()  # Use train data's median for consistency\n        new_test_data[col].fillna(median_value, inplace=True)\n    else:\n        print(f\"'{col}' not found in test data.\")\n\n# Step 7: Handle Date Columns (Fill with placeholder or special handling)\nfor col in date_columns:\n    new_train_data[col].fillna(\"Unknown Date\", inplace=True)\n    new_test_data[col].fillna(\"Unknown Date\", inplace=True)\n\n# Ensure the selected columns are numeric\nfor col in knn_columns:\n    new_train_data[col] = pd.to_numeric(new_train_data[col], errors='coerce')\n    new_test_data[col] = pd.to_numeric(new_test_data[col], errors='coerce')\n\n# Check for all-NaN columns and fill temporarily\nfor col in knn_columns:\n    if new_train_data[col].isnull().all():\n        new_train_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n    if new_test_data[col].isnull().all():\n        new_test_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n\n# Initialize the KNN Imputer\nimputer = KNNImputer(n_neighbors=5)\n\n# Apply KNN Imputation on Train Data\nknn_train_data = pd.DataFrame(\n    imputer.fit_transform(new_train_data[knn_columns]),\n    columns=knn_columns,\n    index=new_train_data.index\n)\nnew_train_data[knn_columns] = knn_train_data\n\n# Apply KNN Imputation on Test Data\nknn_test_data = pd.DataFrame(\n    imputer.transform(new_test_data[knn_columns]),\n    columns=knn_columns,\n    index=new_test_data.index\n)\nnew_test_data[knn_columns] = knn_test_data\n\nprint(\"KNN Imputation applied successfully!\")\n\n\n# Step 9: Create Indicator Columns for Missing Data\nfor col in new_train_data.columns:\n    if new_train_data[col].isnull().any():\n        new_train_data[f'{col}_missing'] = new_train_data[col].isnull().astype(int)\n        new_test_data[f'{col}_missing'] = new_test_data[col].isnull().astype(int)\n\n# Print completion message\nprint(\"Missing values handled successfully with a refined strategy.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:05:38.131101Z","iopub.execute_input":"2024-12-27T20:05:38.131384Z","iopub.status.idle":"2024-12-27T20:06:17.743331Z","shell.execute_reply.started":"2024-12-27T20:05:38.131363Z","shell.execute_reply":"2024-12-27T20:06:17.742402Z"}},"outputs":[{"name":"stdout","text":"'matched_score' not found in test data.\nKNN Imputation applied successfully!\nMissing values handled successfully with a refined strategy.\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor, Pool\nimport re\nfrom ast import literal_eval\nfrom sklearn.decomposition import TruncatedSVD, NMF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:06:37.633694Z","iopub.execute_input":"2024-12-27T20:06:37.633992Z","iopub.status.idle":"2024-12-27T20:06:37.638730Z","shell.execute_reply.started":"2024-12-27T20:06:37.633969Z","shell.execute_reply":"2024-12-27T20:06:37.637694Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:06:40.849209Z","iopub.execute_input":"2024-12-27T20:06:40.849519Z","iopub.status.idle":"2024-12-27T20:06:40.853861Z","shell.execute_reply.started":"2024-12-27T20:06:40.849493Z","shell.execute_reply":"2024-12-27T20:06:40.853067Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"# class TextProcessor:\n#     def __init__(self, max_features=200):\n#         self.tfidf_models = {}\n#         self.count_models = {}\n#         self.svd_models = {}\n#         self.max_features = max_features\n\n#         # Initialize lemmatizer and stopwords\n#         self.stop_words = set(stopwords.words('english'))\n        \n#         # Example for domain-specific abbreviations\n#         self.abbreviation_dict = {\n#             \"etc\": \"et cetera\",  # expanding abbreviations as an example\n#             \"info\": \"information\"\n#             # You can add more abbreviations or domain-specific terms here\n#         }\n\n#     def expand_abbreviations(self, text):\n#         # Replace abbreviations with full form\n#         for abbr, full_form in self.abbreviation_dict.items():\n#             text = text.replace(abbr, full_form)\n#         return text\n    \n#     def clean_text(self, text):\n#         if pd.isna(text):\n#             return ''\n#         text = str(text).lower()\n#         # Expand abbreviations\n#         text = self.expand_abbreviations(text)\n#         text = re.sub(r'[^\\w\\s]', ' ', text)\n#         text = ' '.join([word for word in text.split() if word not in self.stop_words])\n#         text = re.sub(r'\\s+', ' ', text).strip()\n#         return text\n    \n#     def process_list(self, text):\n#         if pd.isna(text) or text == '':\n#             return []\n#         try:\n#             items = literal_eval(text)\n#             return [self.clean_text(item) for item in items]\n#         except:\n#             return [self.clean_text(item) for item in str(text).split(',')]\n\n#     def fit_transform_text(self, texts, feature_name):\n#         processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n#         # TF-IDF features\n#         self.tfidf_models[feature_name] = TfidfVectorizer(\n#             max_features=self.max_features,\n#             ngram_range=(1, 2),\n#             stop_words='english'\n#         )\n#         tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        \n#         # Count features\n#         self.count_models[feature_name] = CountVectorizer(\n#             max_features=self.max_features//2,\n#             ngram_range=(1, 2),\n#             stop_words='english'\n#         )\n#         count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        \n#         # Reduce dimensionality\n#         self.svd_models[feature_name] = TruncatedSVD(n_components=50)\n#         svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        \n#         return np.hstack([\n#             tfidf_matrix.toarray(),\n#             count_matrix.toarray(),\n#             svd_matrix\n#         ])\n\n#     def transform_text(self, texts, feature_name):\n#         processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n#         tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n#         count_matrix = self.count_models[feature_name].transform(processed_texts)\n#         svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        \n#         return np.hstack([\n#             tfidf_matrix.toarray(),\n#             count_matrix.toarray(),\n#             svd_matrix\n#         ])\n\n# class FeatureEngineer:\n#     def __init__(self):\n#         self.text_processor = TextProcessor()\n#         self.label_encoders = {}\n#         self.scaler = StandardScaler()\n\n#     def extract_required_experience(self, experience_required):\n#         try:\n#             # Match phrases like 'At least 1 year', '1 to 3 years', etc.\n#             if \"to\" in experience_required:  # e.g., '1 to 3 years'\n#                 match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', experience_required)\n#                 if match:\n#                     min_years = int(match.group(1))\n#                     # max_years = int(match.group(2))\n#                     return min_years  # You can return min_years or max_years instead if needed\n#             else:\n#                 match = re.search(r'(\\d+)\\s*year', experience_required)  # e.g., 'At least 5 year(s)'\n#                 if match:\n#                     return int(match.group(1))\n#         except Exception as e:\n#             # print(f\"Error extracting experience from '{experience_required}': {e}\")\n#             return 0\n        \n#     def extract_years_experience(self, row):\n#         try:\n#             start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n#             end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n#             if not end_years:\n#                 end_years = [2024]  # Current year for ongoing positions\n#             return sum(e - s for s, e in zip(start_years, end_years))\n#         except:\n#             return 0\n    \n#     def extract_education_level(self, degree):\n#         if pd.isna(degree):\n#             return 0\n#         degree = str(degree).lower()\n#         if 'phd' in degree or 'doctorate' in degree:\n#             return 4\n#         elif 'master' in degree:\n#             return 3\n#         elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n#             return 2\n#         elif 'diploma' in degree or 'certificate' in degree:\n#             return 1\n#         return 0\n\n#     def match_experience(self, df):\n#         # Loop over each row in the DataFrame\n#         for index, row in df.iterrows():\n#             # Extract required and actual experience\n#             required_experience = self.extract_required_experience(row['experiencere_requirement'])\n#             actual_experience = self.extract_years_experience(row)  # Assuming this is a method in your class\n\n#             # Update the 'experience_match' column based on comparison\n#             if actual_experience >= required_experience:\n#                 df.at[index, 'experience_match'] = 1  # Match\n#             else:\n#                 df.at[index, 'experience_match'] = 0  # Does not match\n\n#     def transform(self, df, is_train=True):\n#         feature_dict = {}\n        \n#         # Experience features\n#         feature_dict['total_experience'] = df.apply(self.extract_years_experience, axis=1)\n#         feature_dict['education_level'] = df['degree_names'].apply(self.extract_education_level)\n#         feature_dict['num_companies'] = df['professional_company_names'].str.count(',').fillna(0) + 1\n#         feature_dict['num_skills'] = df['skills'].str.count(',').fillna(0) + 1\n#         feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        \n#         # Process text features\n#         text_features = ['skills', 'career_objective', 'responsibilities', 'educational_institution_name']\n#         all_text_features = {}\n        \n#         for feature in text_features:\n#             if is_train:\n#                 text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n#             else:\n#                 text_matrix = self.text_processor.transform_text(df[feature], feature)\n            \n#             for i in range(text_matrix.shape[1]):\n#                 all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n        \n#         # Skills matching scores\n#         df['skills_required'] = df['skills_required'].fillna('')\n#         df['skills'] = df['skills'].fillna('')\n#         required_skills = df['skills_required'].apply(self.text_processor.process_list)\n#         candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n#         feature_dict['skills_match_ratio'] = [\n#             len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n#             for req, cand in zip(required_skills, candidate_skills)\n#         ]\n\n#         # required_experience = df['experience_requirement'].apply(self.extract_required_experience, axis=1)\n#         # # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n#         # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n        \n#         # feature_dict['experience_match_ratio'] = [\n#         #     len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n#         #     for req, cand in zip(required_experience, candidate_experience)\n#         # ]\n\n#         df['experience_match'] = 0\n#         self.match_experience(df)\n#         feature_dict['experience_match_ratio'] = df['experience_match']\n        \n#         # Convert features to DataFrame\n#         feature_df = pd.DataFrame(feature_dict, index=df.index)\n#         text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n        \n#         # Scale numerical features\n#         if is_train:\n#             feature_df = pd.DataFrame(\n#                 self.scaler.fit_transform(feature_df),\n#                 columns=feature_df.columns,\n#                 index=feature_df.index\n#             )\n#         else:\n#             feature_df = pd.DataFrame(\n#                 self.scaler.transform(feature_df),\n#                 columns=feature_df.columns,\n#                 index=feature_df.index\n#             )\n        \n#         return pd.concat([feature_df, text_feature_df], axis=1)\n\n# def train_model():\n#     train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n#     test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n#     train_df = new_train_data\n#     test_df = new_test_data\n\n#     fe = FeatureEngineer()\n\n#     print(\"Transforming train data...\")\n#     train_features = fe.transform(train_df, is_train=True)\n#     print(\"Transforming test data...\")\n#     test_features = fe.transform(test_df, is_train=False)\n\n#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n#     cv_scores = []\n#     test_preds = np.zeros(len(test_df))\n\n#     params = {\n#         'objective': 'regression_l2',\n#         'metric': 'l2',\n#         'boosting_type': 'gbdt',\n#         'device': 'gpu',\n#         'num_leaves': 100, #31\n#         'learning_rate': 0.01,\n#         'feature_fraction': 0.8, #0.8\n#         'bagging_fraction': 0.8, #0.8\n#         'bagging_freq': 8, #5\n#         'reg_alpha': 0.1,\n#         'reg_lambda': 0.1,\n#         'min_child_samples': 30, #20,\n#         'max_bin': 255,\n#         'gpu_use_dp': True,  # Enable multi-GPU training\n#         'tree_learner': 'data_parallel',  # Use multi-GPU setup\n#         'verbose': -1\n#     }\n\n#     for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n#         print(f\"Training fold {fold + 1}\")\n#         X_train = train_features.iloc[train_idx]\n#         y_train = train_df.iloc[train_idx]['matched_score']\n#         X_val = train_features.iloc[val_idx]\n#         y_val = train_df.iloc[val_idx]['matched_score']\n\n#         train_data = lgbm.Dataset(X_train, label=y_train)\n#         val_data = lgbm.Dataset(X_val, label=y_val)\n        \n#         model = lgbm.train(\n#             params,\n#             train_data,\n#             num_boost_round=2000,\n#             valid_sets=[train_data, val_data],\n#             callbacks=[\n#                 lgbm.early_stopping(stopping_rounds=100),\n#                 lgbm.log_evaluation(100)\n#             ]\n#         )\n\n#         val_preds = model.predict(X_val)\n#         fold_score = mean_squared_error(y_val, val_preds)\n#         cv_scores.append(fold_score)\n        \n#         test_preds += model.predict(test_features) / kf.n_splits\n\n#     print(f\"CV MSE: {np.mean(cv_scores):.6f} Â± {np.std(cv_scores):.6f}\")\n\n#     submission = pd.DataFrame({\n#         'ID': test_df['ID'],\n#         'matched_score': test_preds\n#     })\n#     submission.to_csv('submission.csv', index=False)\n#     print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T19:34:43.009128Z","iopub.execute_input":"2024-12-27T19:34:43.009434Z","iopub.status.idle":"2024-12-27T19:34:43.034343Z","shell.execute_reply.started":"2024-12-27T19:34:43.009408Z","shell.execute_reply":"2024-12-27T19:34:43.033443Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=300):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.nmf_models = {}\n        self.max_features = max_features\n\n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', 'NUM', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 3),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features // 2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:06:47.834136Z","iopub.execute_input":"2024-12-27T20:06:47.834413Z","iopub.status.idle":"2024-12-27T20:06:47.843555Z","shell.execute_reply.started":"2024-12-27T20:06:47.834392Z","shell.execute_reply":"2024-12-27T20:06:47.842708Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport ast\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Feature Weights from Importance Plot\nFEATURE_WEIGHTS = {\n    'skills_required': 0.12,\n    'locations': 0.10,\n    'experience_requirement': 0.09,\n    'job_position_name': 0.08,\n    'educational_requirements': 0.07,\n    'major_field_of_studies': 0.06,\n    'responsibilities.1': 0.05,\n    'passing_years': 0.05,\n    'career_objective': 0.04,\n    'skills': 0.03,\n    'age_requirement': 0.03,\n    'start_dates': 0.02,\n    'responsibilities': 0.02,\n    'educational_match': 0.05,\n    'field_match': 0.05,\n    'edu_field_match': 0.05,\n}\n\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.scaler = StandardScaler()\n        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n\n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]\n            experiences = [e - s for s, e in zip(start_years, end_years)]\n            return {\n                'total_experience': sum(experiences),\n                'max_experience': max(experiences) if experiences else 0,\n                'num_positions': len(experiences)\n            }\n        except:\n            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n\n    def clean_education_result(self, result_str):\n        if pd.isna(result_str):\n            return 0\n        try:\n            if result_str.startswith('['):\n                result_str = literal_eval(result_str)[0]\n            result_str = str(result_str).upper()\n            if result_str in ['N/A', 'NONE', 'NAN', '']:\n                return 0\n            result_str = result_str.replace('%', '')\n            return float(result_str)\n        except:\n            return 0\n\n    def extract_education_features(self, row):\n        try:\n            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n            result = self.clean_education_result(row['educational_results'])\n            edu_score = 0\n            if 'phd' in degree or 'doctorate' in degree:\n                edu_score = 4\n            elif 'master' in degree:\n                edu_score = 3\n            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n                edu_score = 2\n            elif 'diploma' in degree or 'certificate' in degree:\n                edu_score = 1\n            return {\n                'education_score': edu_score,\n                'education_result': result,\n                'education_weight': edu_score * (result / 100 if result > 0 else 1)\n            }\n        except:\n            return {\n                'education_score': 0,\n                'education_result': 0,\n                'education_weight': 0\n            }\n\n    def extract_professional_companies(self, row):\n        try:\n            # Attempt to convert the string representation of a list into an actual list using ast.literal_eval\n            company_list = ast.literal_eval(row['professional_company_names']) if not pd.isna(row['professional_company_names']) else []\n        \n            # Filter out 'N/A' values, if present, as they should not contribute to the company count\n            company_list = [company for company in company_list if company != 'N/A']\n        \n            # Calculate the company score as the number of companies\n            company_score = len(company_list)\n        \n            return {\n                'company_score': company_score,\n                'company_names': company_list,\n                'company_weight': company_score * 1  # You can apply any specific logic here for weight if needed\n            }\n        except Exception as e:\n            # Handle any errors in case of incorrect data formats or missing values\n            return {\n                'company_score': 0,\n                'company_names': [],\n                'company_weight': 0\n            }\n\n    def calculate_weighted_feature(self, df):\n        \"\"\"Calculates a composite feature using FEATURE_WEIGHTS.\"\"\"\n        df['composite_feature'] = 0  # Initialize composite feature\n        for feature, weight in FEATURE_WEIGHTS.items():\n            if feature in df.columns:\n                # Convert to numeric, replacing non-numeric entries with 0\n                numeric_feature = pd.to_numeric(df[feature], errors='coerce').fillna(0)\n                df['composite_feature'] += numeric_feature * weight\n        return df\n\n    def transform(self, df, is_train=True):\n        # Numeric features from experience and education\n        exp_features = df.apply(self.extract_years_experience, axis=1)\n        edu_features = df.apply(self.extract_education_features, axis=1)\n        comp_features = df.apply(self.extract_professional_companies, axis=1)\n\n        feature_dict = {}\n        for feat in ['total_experience', 'max_experience', 'num_positions']:\n            feature_dict[feat] = [x[feat] for x in exp_features]\n        for feat in ['education_score', 'education_result', 'education_weight']:\n            feature_dict[feat] = [x[feat] for x in edu_features]\n        feature_dict['company_score'] = [x['company_score'] for x in comp_features]\n\n        # Basic numeric count features\n        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n\n        # Additional *interaction* features\n        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n            np.array(feature_dict['num_positions']) + 0.1\n        )\n        feature_dict['result_x_edu_score'] = (\n            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n        )\n\n        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n\n        # Drop constant columns\n        constant_cols = numeric_df.columns[numeric_df.nunique() <= 1]\n        if len(constant_cols) > 0:\n            print(f\"Dropping constant columns: {list(constant_cols)}\")\n            numeric_df = numeric_df.drop(columns=constant_cols)\n\n        # Power transform or scale numeric features\n        if is_train:\n            try:\n                numeric_arr = self.power_transformer.fit_transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.fit_transform(numeric_df)\n        else:\n            try:\n                numeric_arr = self.power_transformer.transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.transform(numeric_df)\n\n        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n\n        # Text features\n        text_features = [\n            'skills', 'career_objective', 'responsibilities',\n            'educational_institution_name', 'degree_names', 'certification_skills',\n            'major_field_of_studies'\n        ]\n\n        all_text_features = {}\n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n\n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n\n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n\n        # Calculate and add weighted composite feature\n        df = self.calculate_weighted_feature(df)\n        numeric_df['composite_feature'] = df['composite_feature']\n\n        # Final combined feature matrix\n        return pd.concat([numeric_df, text_feature_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:06:52.269143Z","iopub.execute_input":"2024-12-27T20:06:52.269427Z","iopub.status.idle":"2024-12-27T20:06:52.287852Z","shell.execute_reply.started":"2024-12-27T20:06:52.269406Z","shell.execute_reply":"2024-12-27T20:06:52.286923Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"def main():\n    # Load datasets\n    train_df = new_train_data\n    test_df = new_test_data\n\n    # Feature Engineering\n    fe = FeatureEngineer()\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n\n    y = train_df['matched_score'].values\n\n    # LightGBM parameters for Multi-GPU\n    lgb_params = {\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',\n        'gpu_platform_id': 0,  # Set your GPU platform\n        'gpu_device_id': 0,  # Use multiple GPUs (e.g., T4*2)\n        'num_leaves': 64,\n        'learning_rate': 0.01,\n        'feature_fraction': 0.9,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 5,\n        'max_bin': 255,\n        'verbose': -1\n    }\n\n    # Cross-validation and model training\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(len(train_features))\n    test_predictions = np.zeros(len(test_features))\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"Training fold {fold + 1}...\")\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val)\n\n        model = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets=[train_data, val_data],\n            num_boost_round=1000,\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(100)\n            ]\n        )\n        oof_predictions[val_idx] = model.predict(X_val)\n        test_predictions += model.predict(test_features) / kf.n_splits\n\n    # Evaluate and save predictions\n    mse = mean_squared_error(y, oof_predictions)\n    print(f\"OOF MSE: {mse:.6f}\")\n\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_predictions\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission saved to submission.csv\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:06:57.052595Z","iopub.execute_input":"2024-12-27T20:06:57.052896Z","iopub.status.idle":"2024-12-27T20:06:57.060734Z","shell.execute_reply.started":"2024-12-27T20:06:57.052873Z","shell.execute_reply":"2024-12-27T20:06:57.059672Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:07:00.325318Z","iopub.execute_input":"2024-12-27T20:07:00.325629Z","iopub.status.idle":"2024-12-27T20:10:53.192333Z","shell.execute_reply.started":"2024-12-27T20:07:00.325604Z","shell.execute_reply":"2024-12-27T20:10:53.191353Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\nDropping constant columns: ['has_certification']\nTransforming test data...\nDropping constant columns: ['has_certification']\nTraining fold 1...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0123754\tvalid_1's l2: 0.0147227\n[200]\ttraining's l2: 0.00779233\tvalid_1's l2: 0.0119165\n[300]\ttraining's l2: 0.00569734\tvalid_1's l2: 0.010709\n[400]\ttraining's l2: 0.0045078\tvalid_1's l2: 0.0100205\n[500]\ttraining's l2: 0.00376244\tvalid_1's l2: 0.00969854\n[600]\ttraining's l2: 0.00321071\tvalid_1's l2: 0.00948581\n[700]\ttraining's l2: 0.00278412\tvalid_1's l2: 0.00934227\n[800]\ttraining's l2: 0.00244068\tvalid_1's l2: 0.00926335\n[900]\ttraining's l2: 0.00213095\tvalid_1's l2: 0.00916294\n[1000]\ttraining's l2: 0.00187703\tvalid_1's l2: 0.0090745\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's l2: 0.00187703\tvalid_1's l2: 0.0090745\nTraining fold 2...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0125333\tvalid_1's l2: 0.0151882\n[200]\ttraining's l2: 0.00779325\tvalid_1's l2: 0.0117128\n[300]\ttraining's l2: 0.00561003\tvalid_1's l2: 0.0102625\n[400]\ttraining's l2: 0.00444853\tvalid_1's l2: 0.00959972\n[500]\ttraining's l2: 0.00369689\tvalid_1's l2: 0.00923243\n[600]\ttraining's l2: 0.00315306\tvalid_1's l2: 0.00901037\n[700]\ttraining's l2: 0.00274035\tvalid_1's l2: 0.00886266\n[800]\ttraining's l2: 0.00238283\tvalid_1's l2: 0.00878148\n[900]\ttraining's l2: 0.00208469\tvalid_1's l2: 0.00871527\n[1000]\ttraining's l2: 0.00182802\tvalid_1's l2: 0.00865851\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's l2: 0.00182802\tvalid_1's l2: 0.00865851\nTraining fold 3...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.012547\tvalid_1's l2: 0.0145097\n[200]\ttraining's l2: 0.00784282\tvalid_1's l2: 0.0110651\n[300]\ttraining's l2: 0.00569203\tvalid_1's l2: 0.00962779\n[400]\ttraining's l2: 0.00452367\tvalid_1's l2: 0.00899122\n[500]\ttraining's l2: 0.0037801\tvalid_1's l2: 0.00865026\n[600]\ttraining's l2: 0.00324307\tvalid_1's l2: 0.00845949\n[700]\ttraining's l2: 0.0028143\tvalid_1's l2: 0.00831303\n[800]\ttraining's l2: 0.00245394\tvalid_1's l2: 0.00821657\n[900]\ttraining's l2: 0.00215169\tvalid_1's l2: 0.00813854\n[1000]\ttraining's l2: 0.00189009\tvalid_1's l2: 0.00807447\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's l2: 0.00189009\tvalid_1's l2: 0.00807447\nTraining fold 4...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0121687\tvalid_1's l2: 0.015027\n[200]\ttraining's l2: 0.00767922\tvalid_1's l2: 0.0116787\n[300]\ttraining's l2: 0.00561839\tvalid_1's l2: 0.0102895\n[400]\ttraining's l2: 0.00446103\tvalid_1's l2: 0.00960751\n[500]\ttraining's l2: 0.00372969\tvalid_1's l2: 0.00925881\n[600]\ttraining's l2: 0.00319884\tvalid_1's l2: 0.00905443\n[700]\ttraining's l2: 0.00277029\tvalid_1's l2: 0.00889586\n[800]\ttraining's l2: 0.00241268\tvalid_1's l2: 0.00879398\n[900]\ttraining's l2: 0.00211467\tvalid_1's l2: 0.00871899\n[1000]\ttraining's l2: 0.00186459\tvalid_1's l2: 0.00864243\nDid not meet early stopping. Best iteration is:\n[995]\ttraining's l2: 0.00187597\tvalid_1's l2: 0.00864029\nTraining fold 5...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0123986\tvalid_1's l2: 0.0145012\n[200]\ttraining's l2: 0.00782178\tvalid_1's l2: 0.0111488\n[300]\ttraining's l2: 0.0057213\tvalid_1's l2: 0.00976727\n[400]\ttraining's l2: 0.0045276\tvalid_1's l2: 0.00909976\n[500]\ttraining's l2: 0.00377359\tvalid_1's l2: 0.0087391\n[600]\ttraining's l2: 0.00323529\tvalid_1's l2: 0.00858012\n[700]\ttraining's l2: 0.00279363\tvalid_1's l2: 0.00845316\n[800]\ttraining's l2: 0.00243977\tvalid_1's l2: 0.00835843\n[900]\ttraining's l2: 0.00213413\tvalid_1's l2: 0.00828323\n[1000]\ttraining's l2: 0.00187272\tvalid_1's l2: 0.00823159\nDid not meet early stopping. Best iteration is:\n[999]\ttraining's l2: 0.00187509\tvalid_1's l2: 0.00823154\nOOF MSE: 0.008536\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":130}]}
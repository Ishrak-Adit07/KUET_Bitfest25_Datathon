{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:26:47.210173Z","iopub.execute_input":"2024-12-27T05:26:47.210504Z","iopub.status.idle":"2024-12-27T05:26:54.404132Z","shell.execute_reply.started":"2024-12-27T05:26:47.210480Z","shell.execute_reply":"2024-12-27T05:26:54.403060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom ast import literal_eval\nfrom sklearn.preprocessing import LabelEncoder\nfrom sentence_transformers import SentenceTransformer\nimport lightgbm as lgbm\nimport joblib\n\n# -----------------------------\n#  TextProcessor class\n# -----------------------------\nclass TextProcessor:\n    def __init__(self):\n        # Load the pre-trained SentenceTransformer model once\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    def process_list(self, text):\n        try:\n            if pd.isna(text) or text == '':\n                return []\n            return literal_eval(text)\n        except:\n            return text.split(',')\n    \n    def get_embeddings(self, texts):\n        texts = [str(t) if not pd.isna(t) else '' for t in texts]\n        return self.model.encode(texts)\n\n# -----------------------------\n#  FeatureEngineer class\n# -----------------------------\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.label_encoders = {}\n    \n    def extract_year(self, date_str):\n        if pd.isna(date_str):\n            return None\n        try:\n            return int(re.findall(r'\\d{4}', str(date_str))[0])\n        except:\n            return None\n    \n    def process_dates(self, df):\n        # Experience duration\n        df['experience_years'] = df.apply(\n            lambda x: self.extract_year(x['end_dates']) - self.extract_year(x['start_dates'])\n            if self.extract_year(x['end_dates']) and self.extract_year(x['start_dates'])\n            else 0, axis=1\n        )\n        return df\n    \n    def process_categorical(self, df, col):\n        if col not in self.label_encoders:\n            self.label_encoders[col] = LabelEncoder()\n            df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].fillna('MISSING'))\n        else:\n            df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].fillna('MISSING'))\n        return df\n    \n    def transform(self, df):\n        # Process dates\n        df = self.process_dates(df)\n        \n        # Process categorical\n        for col in ['degree_names', 'result_types', 'major_field_of_studies']:\n            df = self.process_categorical(df, col)\n        \n        # Get embeddings for text features\n        text_features = ['skills', 'career_objective', 'responsibilities']\n        embedding_cols = {}\n        \n        for feature in text_features:\n            embeddings = self.text_processor.get_embeddings(df[feature])\n            for i in range(embeddings.shape[1]):\n                embedding_cols[f'{feature}_emb_{i}'] = embeddings[:, i]\n        \n        # Concatenate all embeddings at once\n        embedding_df = pd.DataFrame(embedding_cols, index=df.index)\n        df = pd.concat([df, embedding_df], axis=1)\n        \n        # Skills matching score\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        df['skills_match_ratio'] = [\n            len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n            for req, cand in zip(required_skills, candidate_skills)\n        ]\n        \n        return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:26:54.405803Z","iopub.execute_input":"2024-12-27T05:26:54.406140Z","iopub.status.idle":"2024-12-27T05:27:17.992798Z","shell.execute_reply.started":"2024-12-27T05:26:54.406110Z","shell.execute_reply":"2024-12-27T05:27:17.991797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n#  TextProcessor class\n# -----------------------------\nclass TextProcessor:\n    def __init__(self):\n        # Load the pre-trained SentenceTransformer model once\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    def process_list(self, text):\n        try:\n            if pd.isna(text) or text == '':\n                return []\n            return literal_eval(text)\n        except:\n            return text.split(',')\n    \n    def get_embeddings(self, texts):\n        texts = [str(t) if not pd.isna(t) else '' for t in texts]\n        return self.model.encode(texts)\n\n# -----------------------------\n#  FeatureEngineer class\n# -----------------------------\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.label_encoders = {}\n    \n    def extract_year(self, date_str):\n        if pd.isna(date_str):\n            return None\n        try:\n            return int(re.findall(r'\\d{4}', str(date_str))[0])\n        except:\n            return None\n    \n    def process_dates(self, df):\n        # Experience duration\n        df['experience_years'] = df.apply(\n            lambda x: self.extract_year(x['end_dates']) - self.extract_year(x['start_dates'])\n            if self.extract_year(x['end_dates']) and self.extract_year(x['start_dates'])\n            else 0, axis=1\n        )\n        return df\n    \n    def process_categorical(self, df, col):\n        if col not in self.label_encoders:\n            self.label_encoders[col] = LabelEncoder()\n            df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].fillna('MISSING'))\n        else:\n            df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].fillna('MISSING'))\n        return df\n    \n    def transform(self, df):\n        # Process dates\n        df = self.process_dates(df)\n        \n        # Process categorical\n        for col in ['degree_names', 'result_types', 'major_field_of_studies']:\n            df = self.process_categorical(df, col)\n        \n        # Get embeddings for text features\n        text_features = ['skills', 'career_objective', 'responsibilities']\n        embedding_cols = {}\n        \n        for feature in text_features:\n            embeddings = self.text_processor.get_embeddings(df[feature])\n            for i in range(embeddings.shape[1]):\n                embedding_cols[f'{feature}_emb_{i}'] = embeddings[:, i]\n        \n        # Concatenate all embeddings at once\n        embedding_df = pd.DataFrame(embedding_cols, index=df.index)\n        df = pd.concat([df, embedding_df], axis=1)\n        \n        # Skills matching score\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        df['skills_match_ratio'] = [\n            len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n            for req, cand in zip(required_skills, candidate_skills)\n        ]\n        \n        return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:27:26.042149Z","iopub.execute_input":"2024-12-27T05:27:26.042999Z","iopub.status.idle":"2024-12-27T05:27:26.055216Z","shell.execute_reply.started":"2024-12-27T05:27:26.042964Z","shell.execute_reply":"2024-12-27T05:27:26.054062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Data\ntrain_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n\n# Initialize the Feature Engineer\nfe = FeatureEngineer()\n\n# Transform the data\nprint(\"Transforming train data...\")\ntrain_df = fe.transform(train_df)\n\n# Select features and target\nemb_cols = [col for col in train_df.columns if 'emb_' in col]  # e.g., 'skills_emb_0', etc.\ncat_cols = [col for col in train_df.columns if 'encoded' in col]\nnum_cols = ['experience_years', 'skills_match_ratio']\nfeature_cols = emb_cols + cat_cols + num_cols\n\nX = train_df[feature_cols]\ny = train_df['matched_score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:27:31.752479Z","iopub.execute_input":"2024-12-27T05:27:31.752859Z","iopub.status.idle":"2024-12-27T05:27:50.851458Z","shell.execute_reply.started":"2024-12-27T05:27:31.752828Z","shell.execute_reply":"2024-12-27T05:27:50.850392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# # Split the data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Create LightGBM datasets\n# train_data = lgbm.Dataset(X_train, label=y_train)\n# val_data = lgbm.Dataset(X_val, label=y_val, reference=train_data)\n# # LightGBM Parameters\n# params = {\n#     'objective': 'regression_l2',\n#     'metric': 'l2',\n#     'num_leaves': 31,\n#     'learning_rate': 0.05,\n#     'feature_fraction': 0.9,\n#     'max_depth': 8,\n#     'reg_alpha': 0.1,\n#     'reg_lambda': 0.1,\n#     'device': 'gpu',  # Specify GPU usage\n#     'gpu_platform_id': 0,\n#     'gpu_device_id': 0,\n#     'gpu_use_dp': True\n# }\n# # Initialize KFold\n# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n# cv_scores = []\n# test_preds = np.zeros(len(test_df))\n\n# # Cross-Validation Loop\n# for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n#     print(f\"Training fold {fold + 1}\")\n    \n#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n#     train_data = lgbm.Dataset(X_train, label=y_train)\n#     val_data = lgbm.Dataset(X_val, label=y_val)\n    \n#     model = lgbm.train(\n#         params,\n#         train_data,\n#         num_boost_round=1000,\n#         valid_sets=[train_data, val_data],\n#         valid_names=['train', 'valid'],\n#         callbacks=[\n#             lgbm.early_stopping(stopping_rounds=50),\n#             lgbm.log_evaluation(100)\n#         ]\n#     )\n    \n#     # Evaluate the fold\n#     val_preds = model.predict(X_val)\n#     fold_mse = mean_squared_error(y_val, val_preds)\n#     cv_scores.append(fold_mse)\n#     print(f\"Fold {fold + 1} MSE: {fold_mse:.6f}\")\n    \n#     # Predict on test data\n#     test_preds += model.predict(test_df[feature_cols]) / kf.n_splits\n\n# # Print overall CV results\n# print(f\"\\nAverage CV MSE: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n\n# # Save the final model from the last fold\n# model.save_model('lgbm_model.txt')\n# print(\"LightGBM model saved to lgbm_model.txt\")\n\n# # Save the FeatureEngineer\n# joblib.dump(fe, 'feature_engineer.pkl')\n# print(\"FeatureEngineer saved to feature_engineer.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport joblib\nimport lightgbm as lgbm\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create LightGBM datasets\ntrain_data = lgbm.Dataset(X_train, label=y_train)\nval_data = lgbm.Dataset(X_val, label=y_val, reference=train_data)\n\n# LightGBM Parameters\nparams = {\n    'objective': 'regression_l2',\n    'metric': 'l2',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'max_depth': 8,\n    'reg_alpha': 0.1,\n    'reg_lambda': 0.1,\n    'device': 'gpu',  # Specify GPU usage\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'gpu_use_dp': True\n}\n\n# Train the model with validation\nmodel = lgbm.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],  # Add validation dataset here\n    valid_names=['train', 'valid'],    # Name the datasets\n    callbacks=[\n        lgbm.early_stopping(stopping_rounds=50),  # Stop if no improvement in 50 rounds\n        lgbm.log_evaluation(100)                 # Log every 100 iterations\n    ]\n)\n\n# Predict on validation set to compute validation MSE\nval_preds = model.predict(X_val)\nval_mse = mean_squared_error(y_val, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n\n# Save the model\nmodel.save_model('lgbm_model.txt')\nprint(\"LightGBM model saved to lgbm_model.txt\")\n\n# Save the FeatureEngineer (fe) pipeline if needed for later preprocessing\njoblib.dump(fe, 'feature_engineer.pkl')\nprint(\"FeatureEngineer saved to feature_engineer.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:42:16.083453Z","iopub.execute_input":"2024-12-27T05:42:16.083841Z","iopub.status.idle":"2024-12-27T05:42:59.266662Z","shell.execute_reply.started":"2024-12-27T05:42:16.083808Z","shell.execute_reply":"2024-12-27T05:42:59.265891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Predict on the validation set\nval_preds = model.predict(X_val)\n\n# Calculate MSE\nval_mse = mean_squared_error(y_val, val_preds)\nprint(f\"Validation MSE: {val_mse:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:27:11.039887Z","iopub.execute_input":"2024-12-27T04:27:11.040267Z","iopub.status.idle":"2024-12-27T04:27:11.095884Z","shell.execute_reply.started":"2024-12-27T04:27:11.040240Z","shell.execute_reply":"2024-12-27T04:27:11.094121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Feature Engineer and Model\nfe = joblib.load('feature_engineer.pkl')\nprint(\"Loaded FeatureEngineer from feature_engineer.pkl\")\nmodel = lgbm.Booster(model_file='lgbm_model.txt')\nprint(\"Loaded LightGBM model from lgbm_model.txt\")\n\n# Load new test data\ntest_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n# Transform the new data\nprint(\"Transforming test data...\")\ntest_df = fe.transform(test_df)\n\n# Select features for inference\nemb_cols = [col for col in test_df.columns if 'emb_' in col]\ncat_cols = [col for col in test_df.columns if 'encoded' in col]\nnum_cols = ['experience_years', 'skills_match_ratio']\nfeature_cols = emb_cols + cat_cols + num_cols\n\n# Predict\npredictions = model.predict(test_df[feature_cols])\n\n# Prepare the submission\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'matched_score': predictions\n})\nsubmission.to_csv('inference_predictions_2.csv', index=False)\nprint(\"Predictions saved to inference_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:28:20.435441Z","iopub.execute_input":"2024-12-27T04:28:20.435733Z","iopub.status.idle":"2024-12-27T04:28:24.890278Z","shell.execute_reply.started":"2024-12-27T04:28:20.435711Z","shell.execute_reply":"2024-12-27T04:28:24.889502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # submission = pd.DataFrame({\n    #     'ID': test_df['ID'],\n    #     'matched_score': test_preds\n    # })\n    # submission.to_csv('submission.csv', index=False)\n    # print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T03:57:43.316908Z","iopub.execute_input":"2024-12-27T03:57:43.317304Z","iopub.status.idle":"2024-12-27T03:57:43.628984Z","shell.execute_reply.started":"2024-12-27T03:57:43.317279Z","shell.execute_reply":"2024-12-27T03:57:43.627841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
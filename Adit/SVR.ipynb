{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:03.386910Z","iopub.execute_input":"2024-12-27T18:12:03.387215Z","iopub.status.idle":"2024-12-27T18:12:03.393375Z","shell.execute_reply.started":"2024-12-27T18:12:03.387195Z","shell.execute_reply":"2024-12-27T18:12:03.392446Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bitfest-datathon-2025/sample_submission.csv\n/kaggle/input/bitfest-datathon-2025/train.csv\n/kaggle/input/bitfest-datathon-2025/test.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Load the train and test datasets\nnew_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\")\nnew_test_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/test.csv\")\n\n# Check basic structure of the datasets\nprint(\"Train Dataset Overview:\\n\")\n# print(new_train_data.info())\nprint(\"\\nTest Dataset Overview:\\n\")\n# print(new_test_data.info())\n\n# new_train_data['responsibilities']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:05.288839Z","iopub.execute_input":"2024-12-27T18:12:05.289140Z","iopub.status.idle":"2024-12-27T18:12:05.466251Z","shell.execute_reply.started":"2024-12-27T18:12:05.289118Z","shell.execute_reply":"2024-12-27T18:12:05.465367Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Overview:\n\n\nTest Dataset Overview:\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Step 1: Handle missing values\nnew_train_data.fillna(\"Unknown\", inplace=True)  # Replace null/None/N/A with \"Unknown\"\n# Alternatively, drop rows/columns with too many missing values\nnew_train_data.dropna(thresh=int(new_train_data.shape[1] * 0.8), axis=0, inplace=True)  # Drop rows with >80% missing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:08.382549Z","iopub.execute_input":"2024-12-27T18:12:08.382847Z","iopub.status.idle":"2024-12-27T18:12:08.418319Z","shell.execute_reply.started":"2024-12-27T18:12:08.382826Z","shell.execute_reply":"2024-12-27T18:12:08.417428Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"new_train_data.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:09.999520Z","iopub.execute_input":"2024-12-27T18:12:09.999852Z","iopub.status.idle":"2024-12-27T18:12:10.034980Z","shell.execute_reply.started":"2024-12-27T18:12:09.999827Z","shell.execute_reply":"2024-12-27T18:12:10.034140Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nimport re\n\n# Step 1: Define column groups based on type and relevance\ntext_columns = ['address','career_objective','locations', 'extra_curricular_activity_types','extra_curricular_organization_links','online_links']\nnumerical_columns = ['matched_score']\ndate_columns = ['start_dates', 'end_dates', 'issue_dates', 'expiry_dates']\n# Define columns for KNN Imputation\nknn_columns = ['age_requirement','experiencere_requirement']  # Add other columns if needed\n# Get all column names in the DataFrame\nall_columns = set(new_train_data.columns)\n\n# Combine all defined column groups\ndefined_columns = set(text_columns + numerical_columns + date_columns + knn_columns)\n\n# Identify columns that are not in the defined groups\ncategorical_columns = list(all_columns - defined_columns)\n\n# Step 2: Preprocessing Function for Age Column\ndef preprocess_age_requirement(column):\n    # Extract numeric ranges and replace non-numeric with NaN\n    def extract_mean_age(val):\n        if isinstance(val, str):\n            # Find ranges like \"Age 25 to 35 years\" and compute the mean\n            match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if match:\n                return (int(match.group(1)) + int(match.group(2))) / 2\n            # Find single ages like \"Age 25 years\"\n            match = re.search(r'(\\d+)', val)\n            if match:\n                return int(match.group(1))\n        return None  # Return None for non-numeric values\n\n    return column.apply(extract_mean_age)\n\ndef preprocess_experience_requirement(column):\n    \"\"\"\n    Preprocess the experience_requirement column by extracting numeric ranges or single values\n    and replacing non-numeric entries with NaN.\n    \"\"\"\n    def extract_mean_experience(val):\n        if isinstance(val, str):\n            # Find ranges like \"3 to 5 years\" and compute the mean\n            range_match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if range_match:\n                return (int(range_match.group(1)) + int(range_match.group(2))) / 2\n            \n            # Find \"At least X year(s)\" or similar patterns\n            at_least_match = re.search(r'At least (\\d+)', val)\n            if at_least_match:\n                return int(at_least_match.group(1))\n            \n            # Find single experience values like \"1 year\" or \"2 year(s)\"\n            single_match = re.search(r'(\\d+)', val)\n            if single_match:\n                return int(single_match.group(1))\n        \n        # Return None for non-numeric or unprocessable entries\n        return None\n\n    # Apply the extraction logic to the entire column\n    return column.apply(extract_mean_experience)\n\n# Step 3: Preprocess Age Requirement\nnew_train_data['age_requirement'] = preprocess_age_requirement(new_train_data['age_requirement'])\nnew_test_data['age_requirement'] = preprocess_age_requirement(new_test_data['age_requirement'])\nnew_train_data['experiencere_requirement'] = preprocess_experience_requirement(new_train_data['experiencere_requirement'])\nnew_test_data['experiencere_requirement'] = preprocess_experience_requirement(new_test_data['experiencere_requirement'])\n\n# Step 4: Impute Categorical Columns\nfor col in categorical_columns:\n    mode_value = new_train_data[col].mode()[0] if not new_train_data[col].mode().empty else \"Not Specified\"\n    new_train_data[col].fillna(mode_value, inplace=True)\n    new_test_data[col].fillna(mode_value, inplace=True)\n\n# Step 5: Impute Text Columns with Placeholder\nfor col in text_columns:\n    new_train_data[col].fillna(\"No Information\", inplace=True)\n    new_test_data[col].fillna(\"No Information\", inplace=True)\n\n\n\nfor col in numerical_columns:\n    if col in new_train_data.columns:  # Check if column exists in train data\n        median_value = new_train_data[col].median()\n        new_train_data[col].fillna(median_value, inplace=True)\n    if col in new_test_data.columns:  # Check if column exists in test data\n        median_value = new_train_data[col].median()  # Use train data's median for consistency\n        new_test_data[col].fillna(median_value, inplace=True)\n    else:\n        print(f\"'{col}' not found in test data.\")\n\n# Step 7: Handle Date Columns (Fill with placeholder or special handling)\nfor col in date_columns:\n    new_train_data[col].fillna(\"Unknown Date\", inplace=True)\n    new_test_data[col].fillna(\"Unknown Date\", inplace=True)\n\n# Ensure the selected columns are numeric\nfor col in knn_columns:\n    new_train_data[col] = pd.to_numeric(new_train_data[col], errors='coerce')\n    new_test_data[col] = pd.to_numeric(new_test_data[col], errors='coerce')\n\n# Check for all-NaN columns and fill temporarily\nfor col in knn_columns:\n    if new_train_data[col].isnull().all():\n        new_train_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n    if new_test_data[col].isnull().all():\n        new_test_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n\n# Initialize the KNN Imputer\nimputer = KNNImputer(n_neighbors=5)\n\n# Apply KNN Imputation on Train Data\nknn_train_data = pd.DataFrame(\n    imputer.fit_transform(new_train_data[knn_columns]),\n    columns=knn_columns,\n    index=new_train_data.index\n)\nnew_train_data[knn_columns] = knn_train_data\n\n# Apply KNN Imputation on Test Data\nknn_test_data = pd.DataFrame(\n    imputer.transform(new_test_data[knn_columns]),\n    columns=knn_columns,\n    index=new_test_data.index\n)\nnew_test_data[knn_columns] = knn_test_data\n\nprint(\"KNN Imputation applied successfully!\")\n\n\n# Step 9: Create Indicator Columns for Missing Data\nfor col in new_train_data.columns:\n    if new_train_data[col].isnull().any():\n        new_train_data[f'{col}_missing'] = new_train_data[col].isnull().astype(int)\n        new_test_data[f'{col}_missing'] = new_test_data[col].isnull().astype(int)\n\n# Print completion message\nprint(\"Missing values handled successfully with a refined strategy.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:11.494805Z","iopub.execute_input":"2024-12-27T18:12:11.495069Z","iopub.status.idle":"2024-12-27T18:12:13.813420Z","shell.execute_reply.started":"2024-12-27T18:12:11.495050Z","shell.execute_reply":"2024-12-27T18:12:13.812682Z"}},"outputs":[{"name":"stdout","text":"'matched_score' not found in test data.\nKNN Imputation applied successfully!\nMissing values handled successfully with a refined strategy.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\nfrom sklearn.svm import SVR\nimport re\nfrom ast import literal_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:28.556519Z","iopub.execute_input":"2024-12-27T18:12:28.556827Z","iopub.status.idle":"2024-12-27T18:12:28.561273Z","shell.execute_reply.started":"2024-12-27T18:12:28.556804Z","shell.execute_reply":"2024-12-27T18:12:28.560328Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:30.422243Z","iopub.execute_input":"2024-12-27T18:12:30.422701Z","iopub.status.idle":"2024-12-27T18:12:30.427579Z","shell.execute_reply.started":"2024-12-27T18:12:30.422662Z","shell.execute_reply":"2024-12-27T18:12:30.426657Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# class TextProcessor:\n#     def __init__(self, max_features=200):\n#         self.tfidf_models = {}\n#         self.count_models = {}\n#         self.svd_models = {}\n#         self.max_features = max_features\n\n#         # Initialize lemmatizer and stopwords\n#         self.stop_words = set(stopwords.words('english'))\n        \n#         # Example for domain-specific abbreviations\n#         self.abbreviation_dict = {\n#             \"etc\": \"et cetera\",  # expanding abbreviations as an example\n#             \"info\": \"information\"\n#             # You can add more abbreviations or domain-specific terms here\n#         }\n\n#     def expand_abbreviations(self, text):\n#         # Replace abbreviations with full form\n#         for abbr, full_form in self.abbreviation_dict.items():\n#             text = text.replace(abbr, full_form)\n#         return text\n    \n#     def clean_text(self, text):\n#         if pd.isna(text):\n#             return ''\n#         text = str(text).lower()\n#         # Expand abbreviations\n#         text = self.expand_abbreviations(text)\n#         text = re.sub(r'[^\\w\\s]', ' ', text)\n#         text = ' '.join([word for word in text.split() if word not in self.stop_words])\n#         text = re.sub(r'\\s+', ' ', text).strip()\n#         return text\n    \n#     def process_list(self, text):\n#         if pd.isna(text) or text == '':\n#             return []\n#         try:\n#             items = literal_eval(text)\n#             return [self.clean_text(item) for item in items]\n#         except:\n#             return [self.clean_text(item) for item in str(text).split(',')]\n\n#     def fit_transform_text(self, texts, feature_name):\n#         processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n#         # TF-IDF features\n#         self.tfidf_models[feature_name] = TfidfVectorizer(\n#             max_features=self.max_features,\n#             ngram_range=(1, 2),\n#             stop_words='english'\n#         )\n#         tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        \n#         # Count features\n#         self.count_models[feature_name] = CountVectorizer(\n#             max_features=self.max_features//2,\n#             ngram_range=(1, 2),\n#             stop_words='english'\n#         )\n#         count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        \n#         # Reduce dimensionality\n#         self.svd_models[feature_name] = TruncatedSVD(n_components=50)\n#         svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        \n#         return np.hstack([\n#             tfidf_matrix.toarray(),\n#             count_matrix.toarray(),\n#             svd_matrix\n#         ])\n\n#     def transform_text(self, texts, feature_name):\n#         processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n#         tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n#         count_matrix = self.count_models[feature_name].transform(processed_texts)\n#         svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        \n#         return np.hstack([\n#             tfidf_matrix.toarray(),\n#             count_matrix.toarray(),\n#             svd_matrix\n#         ])\n\n# class FeatureEngineer:\n#     def __init__(self):\n#         self.text_processor = TextProcessor()\n#         self.label_encoders = {}\n#         self.scaler = StandardScaler()\n\n#     def extract_required_experience(self, experience_required):\n#         try:\n#             # Match phrases like 'At least 1 year', '1 to 3 years', etc.\n#             if \"to\" in experience_required:  # e.g., '1 to 3 years'\n#                 match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', experience_required)\n#                 if match:\n#                     min_years = int(match.group(1))\n#                     # max_years = int(match.group(2))\n#                     return min_years  # You can return min_years or max_years instead if needed\n#             else:\n#                 match = re.search(r'(\\d+)\\s*year', experience_required)  # e.g., 'At least 5 year(s)'\n#                 if match:\n#                     return int(match.group(1))\n#         except Exception as e:\n#             # print(f\"Error extracting experience from '{experience_required}': {e}\")\n#             return 0\n        \n#     def extract_years_experience(self, row):\n#         try:\n#             start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n#             end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n#             if not end_years:\n#                 end_years = [2024]  # Current year for ongoing positions\n#             return sum(e - s for s, e in zip(start_years, end_years))\n#         except:\n#             return 0\n    \n#     def extract_education_level(self, degree):\n#         if pd.isna(degree):\n#             return 0\n#         degree = str(degree).lower()\n#         if 'phd' in degree or 'doctorate' in degree:\n#             return 4\n#         elif 'master' in degree:\n#             return 3\n#         elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n#             return 2\n#         elif 'diploma' in degree or 'certificate' in degree:\n#             return 1\n#         return 0\n\n#     def match_experience(self, df):\n#         # Loop over each row in the DataFrame\n#         for index, row in df.iterrows():\n#             # Extract required and actual experience\n#             required_experience = self.extract_required_experience(row['experiencere_requirement'])\n#             actual_experience = self.extract_years_experience(row)  # Assuming this is a method in your class\n\n#             # Update the 'experience_match' column based on comparison\n#             if actual_experience >= required_experience:\n#                 df.at[index, 'experience_match'] = 1  # Match\n#             else:\n#                 df.at[index, 'experience_match'] = 0  # Does not match\n\n#     def transform(self, df, is_train=True):\n#         feature_dict = {}\n        \n#         # Experience features\n#         feature_dict['total_experience'] = df.apply(self.extract_years_experience, axis=1)\n#         feature_dict['education_level'] = df['degree_names'].apply(self.extract_education_level)\n#         feature_dict['num_companies'] = df['professional_company_names'].str.count(',').fillna(0) + 1\n#         feature_dict['num_skills'] = df['skills'].str.count(',').fillna(0) + 1\n#         feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        \n#         # Process text features\n#         text_features = ['skills', 'career_objective', 'responsibilities', 'educational_institution_name']\n#         all_text_features = {}\n        \n#         for feature in text_features:\n#             if is_train:\n#                 text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n#             else:\n#                 text_matrix = self.text_processor.transform_text(df[feature], feature)\n            \n#             for i in range(text_matrix.shape[1]):\n#                 all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n        \n#         # Skills matching scores\n#         df['skills_required'] = df['skills_required'].fillna('')\n#         df['skills'] = df['skills'].fillna('')\n#         required_skills = df['skills_required'].apply(self.text_processor.process_list)\n#         candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n#         feature_dict['skills_match_ratio'] = [\n#             len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n#             for req, cand in zip(required_skills, candidate_skills)\n#         ]\n\n#         # required_experience = df['experience_requirement'].apply(self.extract_required_experience, axis=1)\n#         # # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n#         # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n        \n#         # feature_dict['experience_match_ratio'] = [\n#         #     len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n#         #     for req, cand in zip(required_experience, candidate_experience)\n#         # ]\n\n#         df['experience_match'] = 0\n#         self.match_experience(df)\n#         feature_dict['experience_match_ratio'] = df['experience_match']\n        \n#         # Convert features to DataFrame\n#         feature_df = pd.DataFrame(feature_dict, index=df.index)\n#         text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n        \n#         # Scale numerical features\n#         if is_train:\n#             feature_df = pd.DataFrame(\n#                 self.scaler.fit_transform(feature_df),\n#                 columns=feature_df.columns,\n#                 index=feature_df.index\n#             )\n#         else:\n#             feature_df = pd.DataFrame(\n#                 self.scaler.transform(feature_df),\n#                 columns=feature_df.columns,\n#                 index=feature_df.index\n#             )\n        \n#         return pd.concat([feature_df, text_feature_df], axis=1)\n\n# def train_model():\n#     train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n#     test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n#     fe = FeatureEngineer()\n\n#     print(\"Transforming train data...\")\n#     train_features = fe.transform(train_df, is_train=True)\n#     print(\"Transforming test data...\")\n#     test_features = fe.transform(test_df, is_train=False)\n\n#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n#     cv_scores = []\n#     test_preds = np.zeros(len(test_df))\n\n#     params = {\n#         'objective': 'regression_l2',\n#         'metric': 'l2',\n#         'num_leaves': 50, #31\n#         'learning_rate': 0.01,\n#         'feature_fraction': 0.8, #0.8\n#         'bagging_fraction': 0.8, #0.8\n#         'bagging_freq': 8, #5\n#         'reg_alpha': 0.1,\n#         'reg_lambda': 0.1,\n#         'min_child_samples': 30, #20,\n#         'max_bin': 255,\n#     }\n\n#     for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n#         print(f\"Training fold {fold + 1}\")\n#         X_train = train_features.iloc[train_idx]\n#         y_train = train_df.iloc[train_idx]['matched_score']\n#         X_val = train_features.iloc[val_idx]\n#         y_val = train_df.iloc[val_idx]['matched_score']\n\n#         train_data = lgbm.Dataset(X_train, label=y_train)\n#         val_data = lgbm.Dataset(X_val, label=y_val)\n        \n#         model = lgbm.train(\n#             params,\n#             train_data,\n#             num_boost_round=2000,\n#             valid_sets=[train_data, val_data],\n#             callbacks=[\n#                 lgbm.early_stopping(stopping_rounds=100),\n#                 lgbm.log_evaluation(100)\n#             ]\n#         )\n\n#         val_preds = model.predict(X_val)\n#         fold_score = mean_squared_error(y_val, val_preds)\n#         cv_scores.append(fold_score)\n        \n#         test_preds += model.predict(test_features) / kf.n_splits\n\n#     print(f\"CV MSE: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n\n#     submission = pd.DataFrame({\n#         'ID': test_df['ID'],\n#         'matched_score': test_preds\n#     })\n#     submission.to_csv('submission.csv', index=False)\n#     print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T10:37:50.743432Z","iopub.execute_input":"2024-12-27T10:37:50.743728Z","iopub.status.idle":"2024-12-27T10:37:50.767924Z","shell.execute_reply.started":"2024-12-27T10:37:50.743708Z","shell.execute_reply":"2024-12-27T10:37:50.767170Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=200):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.max_features = max_features\n\n        # Initialize lemmatizer and stopwords\n        self.stop_words = set(stopwords.words('english'))\n        \n        # Example for domain-specific abbreviations\n        self.abbreviation_dict = {\n            \"etc\": \"et cetera\",  # expanding abbreviations as an example\n            \"info\": \"information\"\n            # You can add more abbreviations or domain-specific terms here\n        }\n\n    def expand_abbreviations(self, text):\n        # Replace abbreviations with full form\n        for abbr, full_form in self.abbreviation_dict.items():\n            text = text.replace(abbr, full_form)\n        return text\n    \n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        # Expand abbreviations\n        text = self.expand_abbreviations(text)\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = ' '.join([word for word in text.split() if word not in self.stop_words])\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    \n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n        # TF-IDF features\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        \n        # Count features\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features//2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        \n        # Reduce dimensionality\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix\n        ])\n\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n\n    def extract_required_experience(self, experience_required):\n        try:\n            # Match phrases like 'At least 1 year', '1 to 3 years', etc.\n            if \"to\" in experience_required:  # e.g., '1 to 3 years'\n                match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', experience_required)\n                if match:\n                    min_years = int(match.group(1))\n                    # max_years = int(match.group(2))\n                    return min_years  # You can return min_years or max_years instead if needed\n            else:\n                match = re.search(r'(\\d+)\\s*year', experience_required)  # e.g., 'At least 5 year(s)'\n                if match:\n                    return int(match.group(1))\n        except Exception as e:\n            # print(f\"Error extracting experience from '{experience_required}': {e}\")\n            return 0\n        \n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]  # Current year for ongoing positions\n            return sum(e - s for s, e in zip(start_years, end_years))\n        except:\n            return 0\n    \n    def extract_education_level(self, degree):\n        if pd.isna(degree):\n            return 0\n        degree = str(degree).lower()\n        if 'phd' in degree or 'doctorate' in degree:\n            return 4\n        elif 'master' in degree:\n            return 3\n        elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n            return 2\n        elif 'diploma' in degree or 'certificate' in degree:\n            return 1\n        return 0\n\n    def match_experience(self, df):\n        # Loop over each row in the DataFrame\n        for index, row in df.iterrows():\n            # Extract required and actual experience\n            required_experience = self.extract_required_experience(row['experiencere_requirement'])\n            actual_experience = self.extract_years_experience(row)  # Assuming this is a method in your class\n\n            # Update the 'experience_match' column based on comparison\n            if actual_experience >= required_experience:\n                df.at[index, 'experience_match'] = 1  # Match\n            else:\n                df.at[index, 'experience_match'] = 0  # Does not match\n\n    def transform(self, df, is_train=True):\n        feature_dict = {}\n        \n        # Experience features\n        feature_dict['total_experience'] = df.apply(self.extract_years_experience, axis=1)\n        feature_dict['education_level'] = df['degree_names'].apply(self.extract_education_level)\n        feature_dict['num_companies'] = df['professional_company_names'].str.count(',').fillna(0) + 1\n        feature_dict['num_skills'] = df['skills'].str.count(',').fillna(0) + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        \n        # Process text features\n        text_features = ['skills', 'career_objective', 'responsibilities', 'educational_institution_name']\n        all_text_features = {}\n        \n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n            \n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n        \n        # Skills matching scores\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        feature_dict['skills_match_ratio'] = [\n            len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n            for req, cand in zip(required_skills, candidate_skills)\n        ]\n\n        # required_experience = df['experience_requirement'].apply(self.extract_required_experience, axis=1)\n        # # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n        # candidate_experience = feature_dict['total_experience'].apply(self.text_processor.process_list)\n        \n        # feature_dict['experience_match_ratio'] = [\n        #     len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n        #     for req, cand in zip(required_experience, candidate_experience)\n        # ]\n\n        df['experience_match'] = 0\n        self.match_experience(df)\n        feature_dict['experience_match_ratio'] = df['experience_match']\n        \n        # Convert features to DataFrame\n        feature_df = pd.DataFrame(feature_dict, index=df.index)\n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n        \n        # Scale numerical features\n        if is_train:\n            feature_df = pd.DataFrame(\n                self.scaler.fit_transform(feature_df),\n                columns=feature_df.columns,\n                index=feature_df.index\n            )\n        else:\n            feature_df = pd.DataFrame(\n                self.scaler.transform(feature_df),\n                columns=feature_df.columns,\n                index=feature_df.index\n            )\n        \n        return pd.concat([feature_df, text_feature_df], axis=1)\n        \ndef train_model():\n    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n    fe = FeatureEngineer()\n\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = []\n    test_preds = np.zeros(len(test_features))\n\n    # SVR parameters\n    svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"Training fold {fold + 1}\")\n\n        # Split training and validation data\n        X_train = train_features.iloc[train_idx]\n        y_train = train_df.iloc[train_idx]['matched_score']\n        X_val = train_features.iloc[val_idx]\n        y_val = train_df.iloc[val_idx]['matched_score']\n\n        # Train SVR\n        svr_model.fit(X_train, y_train)\n\n        # Predict on validation data\n        val_preds = svr_model.predict(X_val)\n        fold_score = mean_squared_error(y_val, val_preds)\n        print(f\"Fold {fold + 1} MSE: {fold_score}\")\n        cv_scores.append(fold_score)\n\n        # Predict on test data and aggregate predictions\n        test_preds += svr_model.predict(test_features) / kf.n_splits\n\n\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_preds\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:34.797096Z","iopub.execute_input":"2024-12-27T18:12:34.797401Z","iopub.status.idle":"2024-12-27T18:12:34.820294Z","shell.execute_reply.started":"2024-12-27T18:12:34.797372Z","shell.execute_reply":"2024-12-27T18:12:34.819486Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T18:12:41.489469Z","iopub.execute_input":"2024-12-27T18:12:41.489818Z","iopub.status.idle":"2024-12-27T18:14:15.282490Z","shell.execute_reply.started":"2024-12-27T18:12:41.489792Z","shell.execute_reply":"2024-12-27T18:14:15.281578Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\nTransforming test data...\nTraining fold 1\nFold 1 MSE: 0.011111402408248493\nTraining fold 2\nFold 2 MSE: 0.010748645408684539\nTraining fold 3\nFold 3 MSE: 0.010921822976660183\nTraining fold 4\nFold 4 MSE: 0.011138967756782381\nTraining fold 5\nFold 5 MSE: 0.010601109120486816\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":19}]}